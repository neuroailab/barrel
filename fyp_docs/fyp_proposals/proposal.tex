\documentclass[12pt]{article}
\usepackage{times}

\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm
\textheight 21cm
\footskip 1.0cm

\newenvironment{sciabstract}{%
\begin{quote} \bf}
{\end{quote}}

\renewcommand\refname{References}

\newcounter{lastnote}
\newenvironment{scilastnote}{%
\setcounter{lastnote}{\value{enumiv}}%
\addtocounter{lastnote}{+1}%
\begin{list}%
{\arabic{lastnote}.}
{\setlength{\leftmargin}{.22in}}
{\setlength{\labelsep}{.5em}}}
{\end{list}}


% Include your paper's title here

\title{Improving understanding of intermediate and higher cortical areas by training models in virtual worlds}

\author
{Chengxu Zhuang$^{1\ast}$, Damian Mrowca$^{2}$, Daniel Yamins$^{1,2,3}$\\
\\
\normalsize{$^{1}$Psychology Department,  Stanford University}\\
\normalsize{$^{2}$Computer Science Department,  Stanford University}\\
\normalsize{$^{3}$Neuroscience Institute,  Stanford University}\\
\normalsize{Jordan Hall Rm 427, Stanford, CA 94305, USA}\\
\\
\normalsize{$^\ast$To whom correspondence should be addressed; E-mail: chengxuz@stanford.edu.}
}

% Include the date command, but leave its argument blank.

\date{}


%%%%%%%%%%%%%%%%% END OF PREAMBLE %%%%%%%%%%%%%%%%



\begin{document}

% Double-space the manuscript.

\baselineskip24pt

% Make the title.

\maketitle

% Place your abstract within the special {sciabstract} environment.

\begin{sciabstract}
In this paper, we explore the use of virtual three-dimensional worlds to train deep neural networks to solve object-related tasks, both at high and intermediate levels of representation.
Specifically, we consider two sensory domains: (1) a typical visual domain, in which the neural network is exposed to visual images created by rendering in a complex three-dimensional scene, and (2) a whisker-like domain in which a vibrissal array is passed across 3-d objects in these same types of scenes, and which the neural network is driven by the array of forces at the base of the vibrissae.
In both cases of input format, we train the networks to solve object-related tasks, including (1) object category recognition, a task putatively engaging high-level abstractions and (2) agent-centered object normal map estimation, a task that is putatively at an intermediate level of abstraction.
After training these models, we then investigate how well these models predict responses in neurons in animal brain areas believed to underly these functions. Specifically, for the visual input case, we seek to determine whether networks trained on both intermediate and high-level tasks lead are improved models of the macaque and human ventral visual stream, compared to existing Deep Neural Networks trained only on high-level tasks. For the whisker input case, we seek to determine whether any of the networks so trained are effective models of higher barrel cortex in rodents, including areas S1 and S2. We also hope that this work will inspire future researchers to utilize the considerable literature about intermediate and high-level behavioral functions to improve predict power of models both in solving sensory tasks and in modeling brains.

\end{sciabstract}

\section*{Introduction}

Brains do remarkable work in actively analyzing environmental information and making decisions based on this information. Among all brain systems, sensory systems are especially concentrated on analyzing some particular environmental information, e.g., light for visual systems, sound for auditory systems, and inputs from whiskers for the rodent somatosensory systems \cite{purves2001neuroscience}.
The goal of these sensory systems is to extract behaviorally useful information from the complex raw input data, a process which could be described as untangling the behavior-related dimensions (such as category) from other orthogonal dimensions (such as translation and rotation of the objects)\cite{yamins2016using}. In this work, we are especially interested in visual systems and the rodent somatosensory systems.

While these two systems radically differ in their input modalities, number of overall neurons, and specific neuronal microcircuits, they share two fundamental characteristics. First, both are \textbf{deep sensory cascades}, consisting of sequences of brain areas (cortical and subcortical), each of which alone is comparatively simple, but which together produce a complex transformation of the input data. Second both are inherently spatially extended (in contrast to, for instance, the olfactory sense, where any fundamental spatial structure on the input is much less clear) \cite{felleman1991distributed}. In this work, we leverage these similarities to produce models for both that share some core common architectural principles.

[Paragraph about ventral history]

In somatosensory systems, researchers have also found evidence showing hierarchical processing for somatosensory input in both human and primates\cite{Pons1987, Inui2004, Iwamura1998}. [Detailed introduction about 2-3 full sentences for the rodent]

A fundamental question that arises from these experimental observations is: what are the underlying algorithms behind these areas? What computations are they doing? These questions naturally take the form of \textbf{computational neuroscience}: e.g. producing a computational model that correctly describes neural responses as a function of stimulus input.

Hierarchical models are also used widely in artificial intelligence to help design better systems for various tasks. Recent work using deep neural networks (DNNs) has achieved significant improvements on object recognition, speech recognition, and numerous other artificial intelligence tasks\cite{Krizhevsky, hinton2012deep, lecun2015deep}. These deep neural networks are all composed of multiple simple neural network layers in series, where the computation in single layer is usually simple but non-linear and stacks of those simple non-linear computations make up a highly complicated non-linear computation. DNNs are also believed to be biologically plausible, and therefore could be good candidates for models of sensory cortical brain systems.

In fact, researchers have also found that the DNNs optimized for performances on object recognition tasks serve as a good model for the primate ventral visual stream \cite{Yamins2013, Yamins2014, Cadieu2014}. [Now, insert 3-4 complete sentences summarizing our existing work on this, specially emphasizing the fact that models can capture V1, V4 and IT pretty well simultaneously.]

\underline{Inspired by this, we are building performance optimized hierarchical models for specific tasks that we think V4 (an intermediate layer in visual systems) in human and primates and mouse somatosensory systems are performing. And after having the models, we could use them to explain the responses of those brain areas.}[Move this later]

However, even the best existing DNN models still capture V4 and IT imperfectly. V4 is believed to encode intermediate level of object features and show strong attentional modulation\cite{Roe2012}, while IT areas are believed to encode high level of object features as object category. 

\underline{And mouse could use their whiskers to detect object shape, position, and texture of object surface\cite{Boubenec2012,Diamond2008,Arabzadeh2005,OConnor2010}.}[Move later] 

Our hypothesis in this work is that by taking into account this intermediate task of normal encoding at an intermediate model level during training, we will produce a better model both of V4 and of IT. Specifically, then, the task that we are interested in for V4 neurons is predicting normals of object surfaces using 2D images as input. Once we have a good model for V4, we could further add some extra layers on top of it to predict the object category from normals. Those extra layers then could be treated as models for IT areas. 

For the rodent somatosensory cortex, we are using a similar task but with input now collected through simulated-whiskers to simulate the input to mouse barrel cortex. Our hypothesis in doing this is that area S1 (primary somatosensory cortex) in the rodent is a normal estimator, while area S2 (secondary somatosensory cortex) is a higher-level object category or shape detector. By explicitly modeling the functions of intermediate layers, we hope that we could have a better model for the whole systems. In fact, unlike the case of visual cortex, where there are many existing models, in rodent barrel cortex, there is significantly less modeling work, and we hope our contribution will be the first substantial such model for S1 and S2.

A core problem in carrying out the plan described above for optimizing deep nets on intermediate-level tasks is the lack of sufficient annotated training data from natural real-world stimuli.   For the visual case, it is extremely challenging to collect high-quality normal estimations of sufficiently many natural scenes that training deep nets on such data is easily effective.   Similarly, it is very challenging to obtain directly what whiskers in a mouse experience, or to build a real-world sensory device mimicking rodent whisker arrays.  We were therefore motivated to create virtual worlds in which these data would be readily available, and train our networks in those virtual worlds. [You should cite existing work (Eigen and Fergus, for example) on training normal extraction networks.]

There has already been some promising work investigating training deep hierarchical models in virtual worlds\cite{Qiu2016,Johnson-Roberson2016}. In the work by Johnson-Roberson and et al.\cite{Johnson-Roberson2016}, virtual worlds have been used to train deep hierarchical models to learn driving. And in our work, we will illustrate that it is possible that we could successfully train a model that performs well on object recognition task both in virtual environment and real world without using any training examples and notations from real world.

As for building a virtual model of the rodent whisker input device, we will make use of recent investigations into the rodent whisker and barrel cortex\cite{Boubenec2012,Diamond2008,Arabzadeh2005,OConnor2010}. Mechanical properties of the rodent whiskers have also been explored in detail \cite{Diamond2008,Quist2014,Towal2011}. Recently, researchers have begun to utilize the known mechanical properties of vibrissae to simulate the responses of whiskers under particular circumstances \cite{Huet2016}. Combining these insights with deep hierarchical models in a virtual world, we will build a mode for both barrel cortex and mouse somatosensory systems.

In the following sections, we will describe the methods we use for building the two virtual worlds and designing/training the neural networks, describe the results obtained both for macaque visual and mouse barrel cortex, and then briefly discuss the implications of these results.

\section*{Methods}

In this section, we will explain how we build the virtual worlds and then how we train models using them.

\subsection*{Virtual worlds}

We build two different virtual worlds for the two systems respectively. For V4 and IT areas modelling, the virtual world we want should be able to generate realistic images showing different objects in various scenes as well as providing the needed notations, which, in our case, is normals on object surfaces and categories of those objects.
In order to make our models trained in this virtual world generalize well to real worlds, we use one of the most popular and realistic game engines, Unity 5 \cite{unity}, to build our virtual world, which we refer to as 3Dworld.
Besides Unity, we also need plenty of realistic 3D objects to be placed in the virtual world. We collected the models from various sources, including Shapenet \cite{Chang2015}, Dosch 3D models \cite{dosch}. To generate 2D images and related notations for training the models, we will first build a scene in this virtual world with reasonable light condition and semantic surroundings (for example, walls and windows for indoor scenes or grass and trees for outdoor scenes).
Following that, we will place different 3D objects in the scene. Currently, the 3D objects are chosen randomly from the whole library of 3D objects we have. Then, we will randomly take images including reasonable number of 3D objects that occupy a reasonable part of the whole image. Finally, the images and normals of objects shown in the image as well as their categories will be used as training examples for deep hierarchical models.

For mouse somatosensory systems, the virtual world should first have a physically accurate models for the the rodent whiskers.
Researchers in computer graphics have done great job modelling human hair or furry objects\cite{Hadap2008, Ward2007}, while their concentration is usually to make tens of thousands of hair strands have reasonable behaviors interacting with each other or with bodies and other objects.
And in our simulations, we are more interested in ensuring the correctness of the mechanical details of every individual whisker. For this virtual world, we use Bullet \cite{wiki:bullet} as physics engine and the 3D objects used are from the same library for the 3Dworld. Every unit is in shape of a cuboid and connected to two neighbors using hinge connections.
Specifically, we use concatenated 25 small units in a chain to model the individual whisker.
To simplify the discussion, we assume the whiskers are laid along X-axis in a 3D space with XYZ axis. All the hinge connections could only rotate in XOY plane. And the base unit is connected to a base ball representing the actual connecting point of whiskers to mouse. The connection between the base unit and base ball is also hinge connection which is only allowed to rotate in ZOY plane.
With only those hinges between every two concatenated units as constraints, different parts of whisker strand would have independent actions as the constraints for two units in large distance would take a long time to actually work. Thus we also add springs between pairs of units that are not neighbors. For example, we add springs connecting every two units that have 2 other units between them.
After adding all those constraints, we could then build an array of individual whisker strands to simulate the whisker array on mouse. And following that, we use the simulated whiskers to sweep over surfaces of objects and collect the responses of the whiskers by measuring how much the several hinges and springs near base unit leave their equilibrium states at every time point. Annotated with normals of the surfaces swept and categories of those objects, those data could be used to train the deep hierarchical models.

\subsection*{Model training}

[Discuss about three core modeling training regimens:  (i) training end-to-end the "final task" (e.g. object recognition, (ii) training pixel-intermediate and then intermediate to end networks, and (iii) doing both simultaneously.

Then explain how we will ask two types of questions: Q1.  are models trained with type (ii) and (iii) better than pure (i) models?  and Q2. How deep should the intermediate task be placed?  How many layers and many examples are needed to learn category from normals, as compared to from raw pixels?

Explain how these ideas above apply to both visual and somatosensory domains.
]

[Then, describe the model architectures, which are more specific to the particular sensory domains.]

\section*{Results}

We will explain this later, not in this proposal.

\section*{Discussion}

We will explain this later, not in this proposal.

\bibliography{scibib}

%\bibliographystyle{Science}
%\bibliographystyle{plainnat}
\bibliographystyle{plain}

\end{document}
