\documentclass[12pt]{article}
\usepackage{times}

\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm
\textheight 21cm
\footskip 1.0cm

\newenvironment{sciabstract}{%
\begin{quote} \bf}
{\end{quote}}

\renewcommand\refname{References}

\newcounter{lastnote}
\newenvironment{scilastnote}{%
\setcounter{lastnote}{\value{enumiv}}%
\addtocounter{lastnote}{+1}%
\begin{list}%
{\arabic{lastnote}.}
{\setlength{\leftmargin}{.22in}}
{\setlength{\labelsep}{.5em}}}
{\end{list}}


% Include your paper's title here

\title{Improving understanding of intermediate and higher cortical areas by training models in virtual worlds}

\author
{Chengxu Zhuang$^{1\ast}$, Damian Mrowca$^{2}$, Daniel Yamins$^{1,2,3}$\\
\\
\normalsize{$^{1}$Psychology Department,  Stanford University}\\
\normalsize{$^{2}$Computer Science Department,  Stanford University}\\
\normalsize{$^{3}$Neuroscience Institute,  Stanford University}\\
\normalsize{Jordan Hall Rm 427, Stanford, CA 94305, USA}\\
\\
\normalsize{$^\ast$To whom correspondence should be addressed; E-mail: chengxuz@stanford.edu.}
}

% Include the date command, but leave its argument blank.

\date{}


%%%%%%%%%%%%%%%%% END OF PREAMBLE %%%%%%%%%%%%%%%%



\begin{document}

% Double-space the manuscript.

\baselineskip24pt

% Make the title.

\maketitle

% Place your abstract within the special {sciabstract} environment.

\begin{sciabstract}
In this paper, we explore the use of virtual three-dimensional worlds to train deep neural networks to solve object-related tasks, both at high and intermediate levels of representation.
Specifically, we consider two sensory domains: (1) a typical visual domain, in which the neural network is exposed to visual images created by rendering in a complex three-dimensional scene, and (2) a whisker-like domain in which a vibrissal array is passed across 3-d objects in these same types of scenes, and which the neural network is driven by the array of forces at the base of the vibrissae.
In both cases of input format, we train the networks to solve object-related tasks, including (1) object category recognition, a task putatively engaging high-level abstractions and (2) agent-centered object normal map estimation, a task that is putatively at an intermediate level of abstraction.
After training these models, we then investigate how well these models predict responses in neurons in animal brain areas believed to underly these functions. Specifically, for the visual input case, we seek to determine whether networks trained on both intermediate and high-level tasks lead are improved models of the macaque and human ventral visual stream, compared to existing Deep Neural Networks trained only on high-level tasks. For the whisker input case, we seek to determine whether any of the networks so trained are effective models of higher barrel cortex in rodents, including areas S1 and S2. We also hope that this work will inspire future researchers to utilize the considerable literature about intermediate and high-level behavioral functions to improve predict power of models both in solving sensory tasks and in modeling brains.

\end{sciabstract}

\section*{Introduction}

Brains do remarkable work in actively analyzing environmental information and making decisions based on this information. Among all brain systems, sensory systems are especially concentrated on analyzing some particular environmental information, e.g., light for visual systems, sound for auditory systems, and inputs from whiskers for mouse somatosensory systems.
The goal of those sensory systems is to extract the useful semantic information from the complex raw input data, which could be described as untangling the behavior-related dimensions (such as category) from other irrelevant dimensions (such as translation and rotation of the objects)\cite{yamins2016using}. And in this work, we are especially interested in visual systems and mouse somatosensory systems.

A lot of work has been done to explore both two systems. While those systems differ from their input data, number of overall neurons, and specific structures as well as organizations, they are believed to have the similarity of consisting of several consecutive regions that are distinguishable on both structures and functions \cite{felleman1991distributed}.
For visual systems, starting from work of Hubel and Wiesel\cite{Hubel1959}, there have been a large number of hierarchical models developed to explain the response patterns of them\cite{riesenhuber1999hierarchical, serre2007feedforward, fukushima1980neocognitron, bengio2009learning, pinto2009high}.
Similarly in somatosensory systems, researchers also find evidence showing hierarchical processing for somatosensory input in both human and primates\cite{Pons1987, Inui2004, Iwamura1998}.

Hierarchical models are also used widely in artificial intelligence to help design better systems for various tasks. The recent work using deep neural networks (DNNs) has achieved significant improvements on object recognition, speech recognition, and numerous of other artificial intelligence tasks\cite{Krizhevsky, hinton2012deep, lecun2015deep}. Those deep neural networks are all composed of multiple simple neural network layers in series, where the computation in single layer is usually simple but non-linear and stacks of those simple non-linear computations finally make up of some highly complicated non-linear computations. Additionally, those models are also believed to be biologically plausible and therefore could be good candidates for models of related brain systems.

Furthermore, researchers have also found that the hierarchical models optimized for performances on object recognition tasks could also serve as a good model for IT areas in primates, which are believed to be the responsible areas for object recognition in brains \cite{Yamins2013, Yamins2014, Cadieu2014}. Inspired by this, we are building performance optimized hierarchical models for specific tasks that we think V4 (an intermediate layer in visual systems) in human and primates and mouse somatosensory systems are performing. And after having the models, we could use them to explain the responses of those brain areas.

Both two areas of interest are poor understood. V4 is believed to encode intermediate level of object features and show strong attentional modulation\cite{Roe2012}, while IT areas are believed to encode high level of object features as object category. And mouse could use their whiskers to detect object shape, position, and texture of object surface\cite{Boubenec2012,Diamond2008,Arabzadeh2005,OConnor2010}. Thus the task that we are interested in for V4 neurons is predicting normals of object surfaces using 2D images as input. Once we have a good model for V4, we could further add some extra layers on top of it to predict the object category from normals. Those extra layers then could be treated as models for IT areas. For mouse somatosensory cortex, we are using the similar task but with input now collected through simulated-whiskers to simulate the input to mouse barrel cortex. Via doing this, we are trying to model S1 (primary somatosensory cortex) as normal predictors and then S2 (secondary somatosensory cortex) as object category detector. With explicitly modeling the functions of intermediate layers, we hope that we could have a better model for the whole systems.

However, for optimizing those deep hierarchical models, one would need a large nubmer of examples with corresponding labels. And in our work, it is either too difficult to collect the corresponding labels (for example, the normals of object surfaces given the 2D images) or the desired example itself (for example, the input from simulated whiskers). Thus we need to create virtual worlds for our tasks and train our models there.

There have already been some works investigating training deep hierarchical models in virtual worlds\cite{Qiu2016,Johnson-Roberson2016}. In the work by Johnson-Roberson and et al.\cite{Johnson-Roberson2016}, virtual worlds have been used to train deep hierarchical models to learn driving. And in our work, we will illustrate that it is possible that we could successfully train a model that performs well on object recognition task both in virtual environment and real world without using any training examples and notations from real world.

As for mouse somatosensory systems, a lot of investigations have been made to mouse whiskers and barrel cortex\cite{Boubenec2012,Diamond2008,Arabzadeh2005,OConnor2010}. Mechanical properties of mouse whiskers have also been explored a lot \cite{Diamond2008,Quist2014,Towal2011}. Recently, researchers started to utilize the mechanical properties known to simulate the responses of whiskers under particular circumstances \cite{Huet2016}. Combining them with deep hierarchical models in a virtual world, we will build a mode for both barrel cortex and mouse somatosensory systems.

In the following sections, we will describe our methods used for both building the virtual worlds and training the models in Methods section. [TODO] Then in Results section, we will illustrate our results from the models and their performances on explaining those systems. [TODO] We will briefly discuss those results in Discuss section.

\section*{Methods}

In this section, we will explain how we build the virtual worlds and then how we train models using them.

\subsection*{Virtual worlds}

We build two different virtual worlds for the two systems respectively. For V4 and IT areas modelling, the virtual world we want should be able to generate realistic images showing different objects in various scenes as well as providing the needed notations, which, in our case, is normals on object surfaces and categories of those objects. 
In order to make our models trained in this virtual world generalize well to real worlds, we use one of the most popular and realistic game engines, Unity 5 \cite{unity}, to build our virtual world, which we refer to as 3Dworld. 
Besides Unity, we also need plenty of realistic 3D objects to be placed in the virtual world. We collected the models from various sources, including Shapenet \cite{Chang2015}, Dosch 3D models \cite{dosch}. To generate 2D images and related notations for training the models, we will first build a scene in this virtual world with reasonable light condition and semantic surroundings (for example, walls and windows for indoor scenes or grass and trees for outdoor scenes). 
Following that, we will place different 3D objects in the scene. Currently, the 3D objects are chosen randomly from the whole library of 3D objects we have. Then, we will randomly take images including reasonable number of 3D objects that occupy a reasonable part of the whole image. Finally, the images and normals of objects shown in the image as well as their categories will be used as training examples for deep hierarchical models.

For mouse somatosensory systems, the virtual world should first have a physically accurate models for the mouse whiskers. 
Researchers in computer graphics have done great job modelling human hair or furry objects\cite{Hadap2008, Ward2007}, while their concentration is usually to make tens of thousands of hair strands have reasonable behaviors interacting with each other or with bodies and other objects. 
And in our simulations, we are more interested in ensuring the correctness of the mechanical details of every individual whisker. For this virtual world, we use Bullet \cite{wiki:bullet} as physics engine and the 3D objects used are from the same library for the 3Dworld. Every unit is in shape of a cuboid and connected to two neighbors using hinge connections.
Specifically, we use concatenated 25 small units in a chain to model the individual whisker.  
To simplify the discussion, we assume the whiskers are laid along X-axis in a 3D space with XYZ axis. All the hinge connections could only rotate in XOY plane. And the base unit is connected to a base ball representing the actual connecting point of whiskers to mouse. The connection between the base unit and base ball is also hinge connection which is only allowed to rotate in ZOY plane.
With only those hinges between every two concatenated units as constraints, different parts of whisker strand would have independent actions as the constraints for two units in large distance would take a long time to actually work. Thus we also add springs between pairs of units that are not neighbors. For example, we add springs connecting every two units that have 2 other units between them.
After adding all those constraints, we could then build an array of individual whisker strands to simulate the whisker array on mouse. And following that, we use the simulated whiskers to sweep over surfaces of objects and collect the responses of the whiskers by measuring how much the several hinges and springs near base unit leave their equilibrium states at every time point. Annotated with normals of the surfaces swept and categories of those objects, those data could be used to train the deep hierarchical models.

\subsection*{Model training}

We will explain this later, not in this proposal.

\section*{Results}

We will explain this later, not in this proposal.

\section*{Discussion}

We will explain this later, not in this proposal.

\bibliography{scibib}

%\bibliographystyle{Science}
%\bibliographystyle{plainnat}
\bibliographystyle{plain}

\end{document}
