In this section, we will show several families of DNNs trained on the dataset, as shown in Figure~\ref{fig_archi}. Different families of DNNs are based on different hypotheses about the computations performed by neurons.
For example, the model with spatial-temporal convolution structure is based on our assumption about neurons integrating spatial-temporal information.
As the temporal dimension of our input data is fixed across different trials, we could use feed-forward convolutional neural networks (CNNs) as well as recurrent neural networks (RNNs).
We will compare different families of networks through their performances while controlling the number of parameters in the model.

\begin{figure}[h]
\centering
\includegraphics [width=1\linewidth]{figures/architectures.pdf}
\vspace{-2mm}
\caption{Network structures illustration for four families of DNNs: \textbf{a)} Illustration for spatiotemporal integrating networks. Convolution is performed to both spatial and temporal dimension, after which, several fully connected layers are applied to the outputs to get the final one dimension output; \textbf{b)} Illustration for networks running temporal integrating before spatial integrating. In temporal integration, convolution is performed to temporal dimension for each whisker. In spatial integration, outputs from each whisker are placed in a 2D grid and then spatial convolution is used to integrate spatial information; \textbf{c)} Illustration for networks running spatial integrating before temporal integrating. Similar to temporal-spatial networks, but now spatial convolution will be applied first and then temporal convolution; \textbf{d)} Illustration for recurrent networks with potential skip or feedback connections. Inputs are split into temporal sequences and presented to the network one by one. The arrow means that the ourput from the starting layer will be sent to the ending layer in the next timestamp.~\label{fig_archi}}
\end{figure}

\subsection{Spatiotemporal integrating networks}

In this family of networks, we use CNNs where convolution is done simultaneously on temporal dimension and spatial dimension, which means that responses from different whiskers across a specific window of time will be combined together in neurons of every layer, while neurons at higher layer will have larger receptive field at both dimensions.
We began with 2D-convolution on temporal and spatial dimension, where the spatial dimension is the indexing of the whiskers through going across the columns and then the rows on the $5\times7$ grid.
We also tried expanding the spatial dimension into two dimensions of $5\times7$ and then doing a 3D-convolution instead, the results will be shown later in this section.

The general structure of the networks includes several convolution layers followed by several fully connected layers. Each categorization is done towards a combination of top, middle, and bottom swipe of the same object under fixed setting.
The same network will be applied to three swipes after which the outputs will be concatenated to give the final categorization label.
For example, one structure in this family consists of 5 convolutional layers and 2 fully connected layers for each swipe with one additional fully connected layer for combining three swipes.
We will call this network as base network, whose structure will be described in detail below.
The convolutional layers of this network are named from \textit{conv1} to \textit{conv5}.
And the filter size of \textit{conv1} is $9\times3$ and that of other convolutional layers is $3\times3$, where the first number is for temporal dimension and the second number is for spatial dimension.
The strides of all convolutional layers for both dimensions are 1. The number of filters are 96, 256, 384, 384, and 256 respectively.
There are max-pooling layers after \textit{conv1}, \textit{conv2}, and \textit{conv5}, called \textit{pool1}, \textit{pool2}, and \textit{pool5}. The filter size of \textit{pool1} is $3\times1$ and the stride of \textit{pool1} is the same.
For \textit{pool2} and \textit{pool5}, the filter size is $3\times3$ while the stride is $2\times2$.
After \textit{pool5}, the output is transformed by two fully connected layers \textit{fc6} and \textit{fc7}. Layer \textit{fc6} has an output shape of 4096 and layer \textit{fc7} has an output shape of 1024.
Finally the three swipes are concatenated together and an additional fully connected layer \textit{fc\_add} is used to predict the category by giving an output with shape of 117.
The network is trained using cross-entropy loss function with Adagrad algorithm~\cite{duchi2011adaptive}.
And the learning rate remains at 0.01 for 15 epoches and then is adjusted to 0.005 for 10 extra epoches.

Several modifications are made to the base network for further explorations. First, two smaller networks with less parameters or even less layers are trained to check whether depth and parameters are necessary.
And as mentioned above, the base network with 3D-convolution rather than 2D-convolution is also trained.
Besides, there is only one additional layer combining the information from three swipes in the base network while the combined information could benefit from more layers of processing, so we also tried another network with more fully connected layers upon combined information.
Furthermore, the base network is only using top, middle, and bottom swipes of a fixed setting for categorization, while in the dataset, there are also three other groups of swipes towards the setting availabel, which means that the categorization could be done to 12 swipes. 
We tried one simple way to use this extra information. It is similar to that of combining information of three swipes. Specifically, we applied the same network to each swipe, concatenated all 12 swipes, and then used one additional layer to get the final label.

\subsection{Temporal integrating before spatial integrating}

Instead of integrating spatial and temporal information at the begining, we could let the networks concentrate on processing temporal information for each individual whisker first and then combine the information from different whiskers in higher layers.
For each swipe, the networks in this family apply the same temporal convolutional network to the temporal signals of each whisker first. 
The outputs of each whisker are then reshaped into one dimension vectors individually and then reorganized in the grid of $5\times7$ according to the actual positions of each whisker with vacancies in the grid filled by zero vectors. 
Another spatial convolution network is applied after that to get a representation including both temporal and spatial information.
Finally, similar to the base network, we concatnate the information from three swipes and use an additional one-layer network to give the final label.

\subsection{Spatial integrating before temporal integrating}

Similar to the previous section, while this time the inputs the spatial convolution is applied to 

\subsection{Recurrent neural networks}
