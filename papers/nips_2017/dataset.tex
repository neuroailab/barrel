With the whisker array as described above, we generated a large-scale dataset of whisker responses to a variety of objects.   

\subsection{Swipe Configuration}
The dataset consists of series of simulated swipes, mimicking one action in which the mouse draws its whiskers across an object.  
During each simulated swipe, a single three-dimensional object moves through the whisker array from front to back at a constant speed.  
Each swipe lasts a total of one second, and data is sampled at 110Hz. 
Swipe scenarios vary both in terms of the identity of the object presented, as well as the position, angle, scale, and speed at which the object is presented.  
To simulate observed mouse whisking behavior~\cite{hobbs2015spatiotemporal}, swipes are performed at each of three heights along the vertical axis (bottom, middle, and top) and at each of 4 positions around the object (0$^{\circ}$, 90$^{\circ}$, 180$^{\circ}$, and 270$^{\circ}$ relative to the vertical axis), for a group of 12 total swipes per object/latent variable setting (Fig. \ref{fig_whiskers}c). 

Latent variables settings are sampled randomly and independently on each group of swipes, with object rotation sampled uniformly within the space of all 3-d rotations, object scale (defined as the length of longest axis) sampled uniformly between 25-135mm, and swipe speed sampled randomly between 77-154mm/s.  
Once these variables are chosen, the object is placed at a position that is chosen uniformly in a  $20 \times 8 \times 20$mm$^{3}$ region centered in front of the whisker array at the chosen vertical height, and is moved along the ray toward the center of the whisker array at the chosen speed. 
The position of the object may be adjusted to avoid collisions with the fixed whisker base ellipsoid during the swipe. 

The data collected during a swipe includes, for each whisker, the forces and torques from all springs connecting to three cuboids most proximate to the base of the whisker, a choice reflecting the known neurophysiological results about the signals measured by whisker follicles~\cite{Quist2014, Huet2016}.  
This data comprises a matrix of shape $110 \times 31 \times 3 \times 2 \times 3$, with dimensions respectively corresponding to: the 110 time samples;  the 31 spatially distinct whiskers; the 3 recorded cuboids; the forces and torques from each cuboid; and the three directional components of force/torque.   


\subsection{Object Set}
The objects used in each swipe are chosen from a subset of the ShapeNet~\cite{Chang2015} dataset, which contains over 50,000 3-dimensional objects, each with a distinct geometry, belonging to 55 categories.
Because the 55 ShapeNet categories are at a variety of levels of within-category semantic similarity, we refined the original 55 categories into a taxonomy of 117 (sub)categories which we felt had a more uniform amount of within-category similarity. 
The distribution of number of ShapeNet objects is very non-uniform across categories, so we randomly subsampled objects from big categories, to ensure a comparatively more balanced distribution.  
This resulted in a choice of 9,981 objects, with most of the 117 categories containing approximately 91 objects exemplars (though there were a few smaller categories, with the smallest category containing 42 exemplars).
To create the final dataset, for every object, 26 independent samples of rotation, scaling, and speed were drawn and the corresponding group of 12 swipes created.   
For each object, the first 24 samples were designated a training subset, while the remaining 2 were added to a testing dataset. 


\subsection{Basic Validation}
To validate that our sensor was at least minimally functional before proceeding to more complex models, we produced a smaller versions of our dataset in which swipes were sampled more densely for two randomly chosen objects (a bear and a duck).  
We also produced multiple easier versions of this dataset in which variation along one or several latent variables was removed. 
We then trained binary SVM classifier to report object identity in these datasets, using just the raw sensor data, and testing classification accuracy on held-out swipes (Fig. \ref{fig_whiskers}d).  We found that with scale and object rotation variability suppressed (but with speed and position variability retained), the sensor was able to nearly perfectly identify the objects.  
However, with all sources of variability, the SVM was just above chance in its performance.  
Thus, we concluded that our virtual whisker array was basically functional, but that unprocessed sensor data cannot be used to directly read out object shape in anything but the most highly controlled circumstances.
As in the case of vision, it is exactly this circumstance that calls for a deep cascade of sensory processing stages. 

